{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP 8: Deep Learning for Grasping\n",
    "\n",
    "**Due date**: April 2, 2021 at 10:45am.\n",
    "\n",
    "**Instructions**: Read and complete the problems below. In this assignment, you should be switched over to a local install. \n",
    "\n",
    "For problems 2 and 3, you will need to install PyTorch on your system, or use the Binder environment.  It is also possible to use Google Colab if you want to use a GPU for training.  To do so, try `pip install torch`. If you have trouble with the install, try `pip install torch==1.8.0+cpu` for a CPU-only install.\n",
    "\n",
    "To submit your assignment, perform the following:\n",
    "\n",
    "1. Double-check that your programs run without error.\n",
    "2. Submit this file, all of your .py files, and some illustrative predicted images (just the ones referenced in your written answers) on Moodle [http:/learn.illinois.edu](http:/learn.illinois.edu).\n",
    "3. If you are using any external libraries other than the ones that are indicated during the installation process, include a README file indicating which library and version you are using.  To be on the safe side, you should include a backup procedure until the graders verify that they are able to support use of that library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Dataset generation\n",
    "\n",
    "We'll augment the example RGB-D images and ground truth grasps from MP7 to see if we can create a dataset with wider scope.  We'll be using the grasps in the dataset `data/grasps/robotiq_85_more_sampled_grasps.json` which adds more (good and bad) grasp samples. We will be changing up the process in MP7 a bit by 1) using greater density of objects, 2) explicit collision checking between the gripper and the object, and 3) examining dense vs sparse prediction.\n",
    "\n",
    "Like in MP7, we will generate world models by sampling base worlds (shelf, table, and box), object placements at random, then generating simulated RGB-D images, and finally generating grasp attribute images.\n",
    "\n",
    "### Problem 1.A\n",
    "\n",
    "In `gen_grasp_worlds`, implement a world sampler that has higher density of objects -- at least 3 per scene.  Use the stable faces logic of MP2, plus collision checking to generate worlds in which the objects rest in reasonable poses and are not colliding. The code provided for you will then save the world to `generated_worlds/world_XXXX.xml`. \n",
    "\n",
    "To visualize the worlds you've generated, run `klampt_browser generated_worlds/world_XXXX.xml`, or just run `klampt_browser` and click on each world file.  Note that you will need to run the program from the MP8 directory for the paths to be loaded properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.B\n",
    "\n",
    "Next, generate camera images from a simulated camera with viewpoint variations. This will help make your predictor less sensitive to the particular viewpoint being used.  Some basic camera poses are predefined for you in the `resources` directory; if you want to edit these poses or set new poses you can uncomment out the \"continue\" marked under the TODO.  For each variation, perturb the position and orientation of the camera within 10-20 cm and 10-20 degrees, but bias these so that the camera still observes the scene.\n",
    "\n",
    "This code will also generate grasp attribute images for your simulated camera but now also include a collision test to only retain grasps that can be reached without a collision.  This means you will need to sample various approach directions and determine whether the gripper collides.  There are two geometries here, one with the gripper open and the other with it closed to the desired width.  When sampling a grasp transform, if the open gripper collides with anything, don't use it.  If the closed gripper collides with anything *except the target object*, don't use it.  If you cannot find a valid approach direction, don't include the grasp in your grasp image.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Deep Learning for Image-Based Prediction\n",
    "\n",
    "Next, you will learn image-based grasp prediction as in MP7, but you'll be using [PyTorch](https://scikit-learn.org/stable/) instead.  This is one of the most pleasant-to-use deep learning toolkits out there, and with a bit of scaffolding code it should work like magic.  (That's not to say that For most of you, you can install it on your local machine using `pip install torch`. You may also be able to use `pip install torch==1.8.0+cpu` if you find that the normal installation fails -- this will just use the CPU for training.  This is not a big deal; our problems are not ridiculously big and you might end up using the CPU to train anyways.  If all else fails, you can boot this notebook up on Binder or even try some GPU-based machines on Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 2.A\n",
    "\n",
    "In problem2.py, `make_patch_dataset` will extract some number of example patches from the image dataset to use for learning.  Just run this once to create a dataset of small images in the `patches` folder.\n",
    "\n",
    "In this learning task, each example point will consist of a feature tensor $x_i$ and a target value $y_i$, and we will predict $y=f(x)$.  We'll just use the concatenation of the color and depth image patches as the feature tensor, which will be a 3D array of size 4 x patch_size x patch_size (the first index indicates R, G, B, and depth channels).  The target value will be a 4-D vector consisting of the score, width, heading, and elevation.  The train and test datasets will be drawn randomly from the patches.  The simple model provided for you will use a linear model followed by a rectified linear unit (ReLU).\n",
    "\n",
    "Next, you will create a convolutional neural network to predict the output.   Create a convolutional neural network that uses two layers of:\n",
    "- [Conv2D](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d), ReLU, and [max-pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d).\n",
    "\n",
    "For the Conv2D layers, start by using a 3x3 convolution, 16 output channels, stride=1, and padding=1. Use 2x2 max-pooling.\n",
    "\n",
    "Follow up the convolutional layers using a fully-connected network consisting of one hidden layer that performs:\n",
    "- flattening,\n",
    "- linear layer, output size 30,\n",
    "- ReLU,\n",
    "- another linear layer, output size 4\n",
    "\n",
    "Train your model using the `train` function given above.  Compare the training process time, training curves, and ultimate RMSE on predicting the score attribute.  Place your response in the area for written answers below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load some basic utility code\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "  \"\"\"Utility NN Module that flattens the incoming tensor.\"\"\"\n",
    "  def forward(self, input):\n",
    "    return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates patches\n",
    "import problem2\n",
    "\n",
    "problem2.generate_patch_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loads the patches and displays the distribution of grasp scores\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import problem2\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "#If you want to experiment with color vs depth, you can try changing this to True...\n",
    "#(Note that your networks will need to support 1 channel rather than 4 if the flag is set!)\n",
    "DEPTH_ONLY = False\n",
    "\n",
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "class AddGaussianOffset(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        offset = random.gauss(self.mean,self.std)\n",
    "        return tensor + torch.ones(tensor.size())*offset\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "class ToTorchDataset:\n",
    "    \"\"\"a little shim code to pack the depth channel onto the color image and\n",
    "    switch around the color channel order\"\"\"\n",
    "    def __init__(self,patch_dataset):\n",
    "        self.patch_dataset = patch_dataset\n",
    "        #TODO: problem 2B: create a Transform object here to do data augmentation\n",
    "        self.color_transform = None\n",
    "        self.depth_transform = None\n",
    "    def __len__(self):\n",
    "        return len(self.patch_dataset)\n",
    "    def __getitem__(self,idx):\n",
    "        res = self.patch_dataset[idx]\n",
    "        color = res['color']\n",
    "        depth = res['depth']\n",
    "        output = res['output']\n",
    "        color = TF.to_tensor(color)\n",
    "        depth = torch.from_numpy(depth[np.newaxis,:,:].astype(np.float32))\n",
    "        if self.color_transform is not None:\n",
    "            color = self.color_transform(TF.to_pil_image(color))\n",
    "            color = TF.to_tensor(color)\n",
    "        if self.depth_transform is not None:\n",
    "            depth = self.depth_transform(depth)\n",
    "        if DEPTH_ONLY:\n",
    "            packed_image = depth\n",
    "        else:\n",
    "            packed_image = torch.cat((color,depth))\n",
    "        return packed_image,output.astype(np.float32)\n",
    "\n",
    "#sanity check\n",
    "def plot_scores(dataset,title):\n",
    "    data = []\n",
    "    for input,output in dataset:\n",
    "        data.append(output[0])\n",
    "    plt.hist(data)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "training_set = ToTorchDataset(problem2.PatchDataset('image_dataset','patch_dataset/train.csv'))\n",
    "testing_set = ToTorchDataset(problem2.PatchDataset(training_set.patch_dataset.dataset,'patch_dataset/test.csv'))\n",
    "testing_set.color_transform = None\n",
    "testing_set.depth_transform = None\n",
    "plot_scores(training_set,'Training set grasp scores')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the best patch, for a sanity check\n",
    "best = training_set.patch_dataset.best_patch\n",
    "score,img,x,y,color_patch,depth_patch = best\n",
    "plt.title(\"Maximum score %.3f\"%score)\n",
    "plt.imshow(training_set.patch_dataset.dataset[img][0])\n",
    "plt.scatter(x, y, s=50, c='red', marker='o')\n",
    "plt.show()\n",
    "plt.imshow(color_patch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda as cuda\n",
    "\n",
    "CUDA = cuda.is_available()\n",
    "SEED = 1\n",
    "BATCH_SIZE = 8\n",
    "PATCH_SIZE = training_set.patch_dataset.patch_size\n",
    "\n",
    "# Use a GPU, i.e. cuda:0 device if it available.\n",
    "if CUDA:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device\",device)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# DataLoader instances will load tensors directly into GPU memory\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_set,batch_size = BATCH_SIZE,shuffle=True,**kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(testing_set,batch_size = BATCH_SIZE,shuffle=True,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap code to get a better sense of what the DataLoader does...\n",
    "for batch in train_loader:\n",
    "    input,output = batch\n",
    "    print(input.__class__.__name__)  #Oh look, it's returning a Tensor\n",
    "    print(input.size(),output.size())  #The leading dimension is 8!  That's the batch size.\n",
    "    print(output)  #this is the 4-D output for 8 examples\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generic training code\n",
    "def train(model, train_loader, test_loader, loss_func, opt, num_epochs=10, calc_rmse=True):\n",
    "  all_training_loss = np.zeros((0,2))\n",
    "  all_training_score_rmse = np.zeros((0,2))\n",
    "  all_test_loss = np.zeros((0,2))\n",
    "  all_test_score_rmse = np.zeros((0,2))\n",
    "  \n",
    "  training_step = 0\n",
    "  training_loss, training_score_rmse = 0.0, 0.0\n",
    "  print_every = 250\n",
    "  \n",
    "  start = time.time()\n",
    "  \n",
    "  for i in range(num_epochs):\n",
    "    epoch_start = time.time() \n",
    "   \n",
    "    model.train()\n",
    "    for images,labels in train_loader:\n",
    "      cpu_labels = labels\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      opt.zero_grad()\n",
    "\n",
    "      preds = model(images)\n",
    "      loss = loss_func(preds, labels)\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      \n",
    "      training_loss += loss.item()\n",
    "      if calc_rmse:\n",
    "          training_score_rmse += np.sum((preds.detach().cpu().numpy()[:,0]-cpu_labels.numpy()[:,0])**2)\n",
    "      \n",
    "      if training_step % print_every == 0:\n",
    "        training_loss /= print_every\n",
    "        training_score_rmse = math.sqrt(training_score_rmse / (print_every*BATCH_SIZE))\n",
    "        \n",
    "        all_training_loss = np.concatenate((all_training_loss, [[training_step, training_loss]]))\n",
    "        all_training_score_rmse = np.concatenate((all_training_score_rmse, [[training_step, training_score_rmse]]))\n",
    "        \n",
    "        print('Epoch %d step %d: Train Loss: %3f, Score RMSE: %3f' % (\n",
    "            i, training_step, training_loss, training_score_rmse))\n",
    "        training_loss, training_score_rmse = 0.0, 0.0\n",
    "        \n",
    "      training_step+=1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      validation_loss, validation_score_rmse = 0.0, 0.0\n",
    "      count = 0\n",
    "      for images,labels in test_loader:\n",
    "        cpu_labels = labels\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        validation_loss+=loss_func(output,labels)\n",
    "        if calc_rmse:\n",
    "            validation_score_rmse+=np.sum((output.detach().cpu().numpy()[:,0]-cpu_labels.numpy()[:,0])**2)\n",
    "        count += 1\n",
    "      validation_loss /= count\n",
    "      validation_score_rmse = math.sqrt(validation_score_rmse/(count*BATCH_SIZE))\n",
    "      \n",
    "      all_test_loss = np.concatenate((all_test_loss, [[training_step, validation_loss]]))\n",
    "      all_test_score_rmse = np.concatenate((all_test_score_rmse, [[training_step, validation_score_rmse]]))\n",
    "      \n",
    "      epoch_time = time.time() - epoch_start\n",
    "      \n",
    "      print('Epoch %d: Test Loss: %3f, Score RMSE: %3f, time: %.1fs' % (\n",
    "          i, validation_loss, validation_score_rmse, epoch_time))\n",
    "      \n",
    "  total_time = time.time() - start\n",
    "  print('Final Test Loss: %3f, Test RMSE: %3f, Total time: %.1fs' % (\n",
    "      validation_loss, validation_score_rmse, total_time))\n",
    "\n",
    "  return {'loss': { 'train': all_training_loss, 'test': all_test_loss },\n",
    "          'score RMSE': { 'train': all_training_score_rmse, 'test': all_test_score_rmse }}\n",
    "\n",
    "def plot_graphs(model_name, metrics):\n",
    "  for metric, values in metrics.items():\n",
    "    for name, v in values.items():\n",
    "      plt.plot(v[:,0], v[:,1], label=name)\n",
    "    plt.title(f'{metric} for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple two-layer fully connected model\n",
    "class TwoLayerModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(TwoLayerModel, self).__init__()\n",
    "    channels = 1 if DEPTH_ONLY else 4\n",
    "    self.net = nn.Sequential(\n",
    "      Flatten(), \n",
    "      nn.Linear(PATCH_SIZE*PATCH_SIZE*channels, 64),\n",
    "      nn.ReLU(), \n",
    "      nn.Linear(64, 4))\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "model_2layer = TwoLayerModel().to(device)\n",
    "\n",
    "# Configure the training\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model_2layer.parameters(), lr=0.001, weight_decay=0.01)\n",
    "training_epochs=4  #this is a simple model, so training will converge in only -34 epochs\n",
    "metrics = train(model_2layer, train_loader, test_loader, loss, optimizer, training_epochs)\n",
    "torch.save(model_2layer.state_dict(), \"model_2layer.pkl\")  #save to disk\n",
    "plot_graphs(\"Two layer model\",metrics)\n",
    "\n",
    "#to load a prior model, run this code rather than optimizing\n",
    "#model_2layer.load_state_dict(torch.load(\"model_2layer.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel,self).__init__()\n",
    "        channels = 1 if DEPTH_ONLY else 4\n",
    "        #TODO: create your model\n",
    "        self.net = nn.Sequential(\n",
    "             Flatten(),\n",
    "             nn.Linear(channels*PATCH_SIZE*PATCH_SIZE, 4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "training_epochs = 10\n",
    "loss = nn.MSELoss()\n",
    "model_cnn = CNNModel().to(device)\n",
    "optimizer = optim.RMSprop(model_cnn.parameters(), lr=0.001, weight_decay=0.01)\n",
    "metrics = train(model_cnn, train_loader, test_loader, loss, optimizer, training_epochs)\n",
    "torch.save(model_cnn.state_dict(), \"model_cnn.pkl\")  #save to disk\n",
    "plot_graphs(\"CNN model\",metrics)\n",
    "\n",
    "#to load a prior model, run this code rather than optimizing\n",
    "#model_cnn.load_state_dict(torch.load(\"model_cnn.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 2.B\n",
    "\n",
    "Now, go back to `make_patch_dataset` and the `ToTorchDataset` class in the cell above to improve the output.  As in MP7, the basic function has a high likelihood of oversampling low-quality regions and undersampling high quality grasps.  First, you should modify `problem2.py` to balance the patch dataset to include more pixels with higher quality grasps.\n",
    "\n",
    "Next, implement data augmentation to ensure that the learner ignores certain characteristics of the images when making its predictions.  This is done using a Transform object from PyTorch, and is described in more detail in the [tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).  You can use the torchvision [ColorJitter](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ColorJitter) function to modify color.  The absolute depth also shouldn't affect the prediction much, so you can get better generalization by adding jitter to the depth channel (such as the `AddGaussianOffset` modifier).  But don't go crazy adding huge offsets, because the depth changes the size of objects relative to the gripper width.\n",
    "\n",
    "Report the changes that you get in the training/testing accuracy in the area for written answers below.\n",
    "\n",
    "The code below does grasp attribute prediction for all pixels one of your test images. Let it run once, and keep the output in your submitted notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates one prediction image for your CNN model\n",
    "#images = training_set.patch_dataset.dataset\n",
    "images = testing_set.patch_dataset.dataset\n",
    "color,depth,transform,grasp_attrs = images[0]\n",
    "patch_radius = training_set.patch_dataset.patch_size//2\n",
    "grasp_preds = np.zeros((480,640,4))\n",
    "for y in range(color.shape[0]):\n",
    "    if y%5 == 0:\n",
    "        print(\"Working on row\",y)\n",
    "    all_patches = []\n",
    "    for x in range(color.shape[1]):\n",
    "        roi = (y-patch_radius,y+patch_radius,x-patch_radius,x+patch_radius)\n",
    "        patch1 = problem2.get_region_of_interest(color,roi)\n",
    "        patch2 = problem2.get_region_of_interest(depth,roi)\n",
    "        if DEPTH_ONLY:\n",
    "            image_packed = patch2[np.newaxis,:,:].astype(np.float32)\n",
    "        else:\n",
    "            image_packed = np.append(np.transpose(patch1,(2,0,1)),patch2[np.newaxis,:,:],0).astype(np.float32)\n",
    "        all_patches.append(image_packed)\n",
    "    model_cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        all_predictions = model_cnn.forward(torch.from_numpy(np.array(all_patches)))\n",
    "    grasp_preds[y] = all_predictions.numpy().reshape((640,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show each of your predictions\n",
    "plt.imshow(grasp_preds[:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(grasp_preds[:,:,1])\n",
    "plt.show()\n",
    "plt.imshow(grasp_preds[:,:,2])\n",
    "plt.show()\n",
    "plt.imshow(grasp_preds[:,:,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Whole-image prediction (IR2 section only)\n",
    "\n",
    "Now that you've gotten the hang of writing networks, we'll try to predict the score over a whole image.  We don't want to use the same patch extraction pipeline as before on a per-pixel basis, since it's expensive and will take minutes.  We also don't want to resort to block hacking like in MP7. Instead, we'll use a method similar to the popular U-net architecture for image segmentation.\n",
    "\n",
    "The idea is that there will be three steps of convolution with downsampling; the resulting 8x downsampled image is assumed to have absorbed enough information from a patch of neighboring pixels to be treated like a feature.  The coarsest level of features will then be\n",
    "- [upsampled](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html) by 2x\n",
    "- [concatenated](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat) with the 4x downsampled image.\n",
    "- Passed through a convolutional layer\n",
    "- Finally, followed by a ReLU (no max-pooling). \n",
    "\n",
    "The result of this level is then treated as the features at the second coarsest level; repeating this process again will give you features for the second finest level, and then repeating a final time will give you features for the output level.\n",
    "\n",
    "The example code below does this for a single stages of paired downsampling / upsampling layers.  Repeat this to complete 3 or more stages. Note that the work needs to be done within the `forward` call, and isn't as simple as calling a Sequence.  Furthermore, for PyTorch to automatically discover the parameters of your network, each module that you use should be an attribute of `self`.\n",
    "\n",
    "Run the subsequent cell to show your predictions.  You may try to improve your results.  If so, what did you try to improve them, and if not, what do you think you could do to improve them?  Give your answers in the spaces for written answers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WholeImageDataset:\n",
    "    def __init__(self,folder,split_start=0,split_end=0.75):\n",
    "        self.dataset = problem2.load_images_dataset(folder)\n",
    "        N = len(self.dataset)\n",
    "        self.dataset = self.dataset[int(N*split_start):int(N*split_end)]\n",
    "        self.color_transform = None\n",
    "        self.depth_transform = None\n",
    "        self.color_transform = transforms.ColorJitter(0.2,0.2,0.5,0.5)\n",
    "        self.depth_transform = AddGaussianOffset(0,0.2)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self,idx):\n",
    "        color,depth,transform,grasp_attrs = self.dataset[idx]\n",
    "        color = TF.to_tensor(color)\n",
    "        depth = torch.from_numpy(depth[np.newaxis,:,:].astype(np.float32))\n",
    "        if self.color_transform is not None:\n",
    "            color = self.color_transform(TF.to_pil_image(color))\n",
    "            color = TF.to_tensor(color)\n",
    "        if self.depth_transform is not None:\n",
    "            depth = self.depth_transform(depth)\n",
    "        if DEPTH_ONLY:\n",
    "            packed_image = depth\n",
    "        else:\n",
    "            packed_image = torch.cat((color,depth))\n",
    "        return packed_image,np.array([grasp_attrs[v] for v in ['score','opening','axis_heading','axis_elevation']],dtype=np.float32)\n",
    "\n",
    "whole_image_training_set = WholeImageDataset('image_dataset',0,0.75)\n",
    "whole_image_testing_set = WholeImageDataset(whole_image_training_set.dataset,0.75,1)\n",
    "whole_image_testing_set.color_transform = None\n",
    "whole_image_testing_set.depth_transform = None\n",
    "whole_image_train_loader = torch.utils.data.DataLoader(whole_image_training_set,batch_size = BATCH_SIZE,shuffle=True,**kwargs)\n",
    "whole_image_test_loader = torch.utils.data.DataLoader(whole_image_testing_set,batch_size = BATCH_SIZE,shuffle=True,**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class FullImagePredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullImagePredictionModel,self).__init__()\n",
    "        channels = 1 if DEPTH_ONLY else 4\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(channels,16,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.upsample_x2 = nn.Upsample(None,(2,2))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv_up1 = nn.Conv2d(16+channels,16,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv_out = nn.Conv2d(16,4,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #this stage reduces the dimension of the image\n",
    "        downsample1 = self.stage1.forward(x)\n",
    "        #this stage upsamples it back to the original resolution\n",
    "        upsample1 = self.upsample_x2.forward(downsample1)\n",
    "        #now concatenate...\n",
    "        merged1 = torch.cat((upsample1,x),1)\n",
    "        #and convolve + ReLU...\n",
    "        conv1 = self.relu.forward(self.conv_up1.forward(merged1))\n",
    "        #one last time!\n",
    "        return self.conv_out.forward(conv1)\n",
    "\n",
    "model_image_prediction = FullImagePredictionModel().to(device)\n",
    "\n",
    "training_epochs=10\n",
    "optimizer = optim.RMSprop(model_image_prediction.parameters(), lr=0.001, weight_decay=0.01)\n",
    "metrics = train(model_image_prediction, whole_image_train_loader, whole_image_test_loader, loss, optimizer, training_epochs, calc_rmse=False)\n",
    "torch.save(model_image_prediction.state_dict(), \"model_image_prediction.pkl\")  #save to disk\n",
    "\n",
    "#to load a prior model, run this code rather than optimizing\n",
    "#model_image_prediction.load_state_dict(torch.load(\"model_image_prediction.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model_image_prediction.forward(whole_image_training_set[20][0].unsqueeze(0)).numpy()[0]\n",
    "    grasp_score = output[0,:,:]\n",
    "    plt.imshow(grasp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written responses\n",
    "\n",
    "### Written response for Problem 2.A\n",
    "\n",
    "Put your answer here.\n",
    "\n",
    "### Written response for Problem 2.B\n",
    "\n",
    "Put your answer here.\n",
    "\n",
    "### Written response for Problem 3\n",
    "\n",
    "Put your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}